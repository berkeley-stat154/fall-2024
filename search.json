[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "[UNDER CONSTRUCTION]\nWe’ll use data from several real world situations in class."
  },
  {
    "objectID": "data.html#data-1",
    "href": "data.html#data-1",
    "title": "Data",
    "section": "Data 1",
    "text": "Data 1"
  },
  {
    "objectID": "data.html#data-2",
    "href": "data.html#data-2",
    "title": "Data",
    "section": "Data 2",
    "text": "Data 2"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "License"
    ]
  },
  {
    "objectID": "staff.html",
    "href": "staff.html",
    "title": "Staff",
    "section": "",
    "text": "Office hours are subject to change.\n\n\n\nInstructor: Song Mei songmei@berkeley.edu Office hours:  - TBA, Evans 387\n\n\nGSI: Ruiqi Zhang rqzhang@berkeley.edu Office hours: - TBA",
    "crumbs": [
      "Staff"
    ]
  },
  {
    "objectID": "units/unit3.html",
    "href": "units/unit3.html",
    "title": "Unit 3: More",
    "section": "",
    "text": "[UNDER CONSTRUCTION]\nThis is an example of using qmd as the source document with pdf as one target. I’ve taken out the qmd stuff that doesn’t seem to render to pdf."
  },
  {
    "objectID": "units/unit3.html#evaluated-python-code-chunk-with-a-plot",
    "href": "units/unit3.html#evaluated-python-code-chunk-with-a-plot",
    "title": "Unit 3: More",
    "section": "Evaluated Python code chunk, with a plot",
    "text": "Evaluated Python code chunk, with a plot\n\n\nCode\nimport numpy as np\nx = np.random.normal(size=100)\nimport matplotlib.pyplot as plt\nplt.hist(x)\nplt.show()\nnp.mean(x)\n\n\n\n\n\n\n\n\n\n0.2338351223732535"
  },
  {
    "objectID": "units/unit3.html#latex",
    "href": "units/unit3.html#latex",
    "title": "Unit 3: More",
    "section": "LaTeX",
    "text": "LaTeX\n\\[\n\\theta = \\int_0^\\infty f(x,\\theta)d\\theta\n\\]"
  },
  {
    "objectID": "units/unit3.html#latex-macro",
    "href": "units/unit3.html#latex-macro",
    "title": "Unit 3: More",
    "section": "LaTeX macro",
    "text": "LaTeX macro\n\nWarning: having include-before-body in the yaml header of this file, as seemingly needed to enable macros, causes extra space at top of page.\n\n\\[\nA = X \\trans Y\n\\]"
  },
  {
    "objectID": "units/macros.html",
    "href": "units/macros.html",
    "title": "",
    "section": "",
    "text": "\\[\n\\newcommand{\\trans}{^\\mathsf{T}}\n\\newcommand{\\eps}{\\epsilon}\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 154/254: Modern Statistical Prediction and Machine Learning",
    "section": "",
    "text": "Ed\n\n  bCourse\n\n  Gradescope\n\n\nNo matching items",
    "crumbs": [
      "Home / Schedule"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Statistics 154/254: Modern Statistical Prediction and Machine Learning",
    "section": "Description",
    "text": "Description\n\n\nInstructor: Song Mei (songmei [at] berkeley.edu)\n\n\nLectures: Tuesday/Thursday 9:30 am - 11:00 am. Etcheverry 3108.\n\n\nOffice Hours: Tuesday 1 pm - 3 pm. Evans 387.\n\n\nGSI: Ruiqi Zhang (rqzhang [at] berkeley.edu)\n\n\nLab sessions Friday 11:00 am - 12:59 pm, Evans 334; 3:00 pm - 4:59 pm, Evans 342.\n\n\nOffice Hours: Thursday 4 pm - 6 pm; Friday 1 pm - 3 pm. Evans 428.\n\n\nThis course will focus on statistical/machine learning methods, data analysis/programming skills. Upon completing this course, the students are expected to be able to 1) build baseline models for real world data analysis problems; 2) implement models using programming languages; 3) draw insights/conclusions from models.",
    "crumbs": [
      "Home / Schedule"
    ]
  },
  {
    "objectID": "index.html#announcement",
    "href": "index.html#announcement",
    "title": "Statistics 154/254: Modern Statistical Prediction and Machine Learning",
    "section": "Announcement",
    "text": "Announcement\n\n\nFirst class: Aug 29, 2024 (Thursday).\n\n\nI will process the CE students during the week of Sep 9. I will prioritize CE students performing well in HW1.\n\n\nWe will use Ed (https://edstem.org/us/courses/61329) for discussions and questions.\n\n\nPlease submit HW using Gradescope (https://www.gradescope.com/courses/811006, code: YRD5EG).\n\n\nPlease find homework and lecture notes on bCourse under “Files”.\n\n\nHW policy: There are in total 3 late days that you can use without penalty towards grade throughout the semester. After that, there will be a 10% deduction on grades of a HW for each late day. The least grade can be dropped counting towards total grades.\n\n\nThe lectures will be recorded through Course Capture. The recordings can be found on bCourse under “Media Gallery”.",
    "crumbs": [
      "Home / Schedule"
    ]
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Statistics 154/254: Modern Statistical Prediction and Machine Learning",
    "section": "Grading",
    "text": "Grading\n\n\nClass attendance is required.\n\n\nHomework per two weeks. There will be 6-7 HWs.\n\n\nIn class mid-term: Oct 24. Logistics: https://docs.google.com/document/d/1VfW1isSq68IwW8TQnZF_3o_SxJ0Yy0sDzI8QVeaSSSw/edit?usp=sharing\n\n\nFinal exam date: Dec 17. Logistics: https://docs.google.com/document/d/1VfW1isSq68IwW8TQnZF_3o_SxJ0Yy0sDzI8QVeaSSSw/edit?usp=sharing\n\n\nFinal grade will be Homework × 40 % + mid-term × 25 % + final × 35 %.\n\n\nHW policy: There are in total three late days that you can use without penalty towards grade throughout the semester. After that, there will be a 10% deduction on grades of a HW for each late day. The least grade can be dropped counting towards total grades.",
    "crumbs": [
      "Home / Schedule"
    ]
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Statistics 154/254: Modern Statistical Prediction and Machine Learning",
    "section": "Topics",
    "text": "Topics\nBasic topics:\n\n\nTasks: Regression. Classification. Dimension reduction. Clustering.\n\n\nAlgorithms: Solving linear systems. Gradient descent. Newton’s method. Power iteration for eigenvalue problems. EM algorithms.\n\n\nOthers: Kernel methods. Regularization. Sample splitting. Resampling methods. Cross validation.\n\n\nAdvanced topics:\n\n\nStatistical learning theory and optimization theory.\n\n\nBagging and Boosting. Tree based models. Neural networks. Bayesian models.\n\n\nOnline learning. Bandit problems.",
    "crumbs": [
      "Home / Schedule"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Statistics 154/254: Modern Statistical Prediction and Machine Learning",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n   Week 1\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Aug 29:\n           \n           Lecture 1 Introduction\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   Week 2\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 3:\n           \n           Lecture 2 Linear model and linear regression\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Sep 5:\n           \n           Lecture 3 Hypothesis testing in linear models (I)\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   Week 3\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 10:\n           \n           Lecture 4 Hypothesis testing in linear models (II)\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Sep 12:\n           \n           Lecture 5 Computational aspects of linear regression; Logistic regression\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   Week 4\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 17:\n           \n           Lecture 6 Logistic regression; Generative modeling approach for classification\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Sep 19:\n           \n           Lecture 7 Generative modeling approach for classification; Support vector machine\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   Week 5\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 24:\n           \n           Lecture 8 Estimation of test error and model selection\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Sep 26:\n           \n           Lecture 9 Zoom Lecture; Regularization\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   Week 5\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Oct 1:\n           \n             No Lecture\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Sep 26:\n           \n           Lecture 10 Zoom lecture; TBA\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   Week 6\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Oct 8:\n           \n           Lecture 11 Zoom lecture; TBA\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Oct 10:\n           \n           Lecture 12 TBA\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   Week 7\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Oct 15:\n           \n           Lecture 13 TBA\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Oct 17:\n           \n           Lecture 14 TBA\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   Week 8\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Oct 22:\n           \n           Lecture 15 TBA\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Oct 24:\n           \n             In class midterm\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n\nNo matching items",
    "crumbs": [
      "Home / Schedule"
    ]
  },
  {
    "objectID": "units/unit2.html",
    "href": "units/unit2.html",
    "title": "Unit 2: Next",
    "section": "",
    "text": "[UNDER CONSTRUCTION]\nThis is an example of using an ipynb file as source rather than qmd. It follows instructions from https://github.com/DS-100/course-notes/README.md.\nLink to the data."
  },
  {
    "objectID": "units/unit2.html#latex",
    "href": "units/unit2.html#latex",
    "title": "Unit 2: Next",
    "section": "\\(\\LaTeX\\)",
    "text": "\\(\\LaTeX\\)\nHere is some \\(\\LaTeX\\). \\[\n\\theta = 7\n\\]"
  },
  {
    "objectID": "units/unit2.html#evaluated-python-code",
    "href": "units/unit2.html#evaluated-python-code",
    "title": "Unit 2: Next",
    "section": "Evaluated Python code",
    "text": "Evaluated Python code\nNote that to get code output shown, the underlying notebook must have executed the code.\n\n\nCode\na=7\nprint(a)\n\n\n7"
  },
  {
    "objectID": "units/unit2.html#callout",
    "href": "units/unit2.html#callout",
    "title": "Unit 2: Next",
    "section": "Callout",
    "text": "Callout\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title."
  },
  {
    "objectID": "units/unit1.html",
    "href": "units/unit1.html",
    "title": "Unit 1: Intro",
    "section": "",
    "text": "[UNDER CONSTRUCTION]\nThis is an example of using qmd as the source document."
  },
  {
    "objectID": "units/unit1.html#evaluated-python-code-chunk-with-a-plot",
    "href": "units/unit1.html#evaluated-python-code-chunk-with-a-plot",
    "title": "Unit 1: Intro",
    "section": "Evaluated Python code chunk, with a plot",
    "text": "Evaluated Python code chunk, with a plot\n\n\nCode\nimport numpy as np\nx = np.random.normal(size=100)\nimport matplotlib.pyplot as plt\nplt.hist(x)\nplt.show()\nnp.mean(x)\n\n\n\n\n\n\n\n\n\n-0.07003087189692557"
  },
  {
    "objectID": "units/unit1.html#latex",
    "href": "units/unit1.html#latex",
    "title": "Unit 1: Intro",
    "section": "\\(\\LaTeX\\)",
    "text": "\\(\\LaTeX\\)\n\\[\n\\theta = \\int_0^\\infty f(x,\\theta)d\\theta\n\\]"
  },
  {
    "objectID": "units/unit1.html#styled-div-via-direct-html",
    "href": "units/unit1.html#styled-div-via-direct-html",
    "title": "Unit 1: Intro",
    "section": "Styled div via direct html",
    "text": "Styled div via direct html\n\nThis content can be styled via the border class."
  },
  {
    "objectID": "units/unit1.html#a-callout",
    "href": "units/unit1.html#a-callout",
    "title": "Unit 1: Intro",
    "section": "A callout",
    "text": "A callout\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title."
  },
  {
    "objectID": "units/unit1.html#tabset",
    "href": "units/unit1.html#tabset",
    "title": "Unit 1: Intro",
    "section": "Tabset",
    "text": "Tabset\n\nRPython\n\n\nThis code is not executed.\nfizz_buzz &lt;- function(fbnums = 1:50) {\n  output &lt;- dplyr::case_when(\n    fbnums %% 15 == 0 ~ \"FizzBuzz\",\n    fbnums %% 3 == 0 ~ \"Fizz\",\n    fbnums %% 5 == 0 ~ \"Buzz\",\n    TRUE ~ as.character(fbnums)\n  )\n  print(output)\n}\n\nfizz_buzz(3)\n\n\nThis code is executed.\n\n\nCode\ndef fizz_buzz(num):\n  if num % 15 == 0:\n    print(\"FizzBuzz\")\n  elif num % 5 == 0:\n    print(\"Buzz\")\n  elif num % 3 == 0:\n    print(\"Fizz\")\n  else:\n    print(num)\n    \nfizz_buzz(3)\n\n\nFizz"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "See Here for full syllabus.\n\nAbout Statistics 154/254\nThis course will focus on statistical/machine learning methods, data analysis/programming skills. Upon completing this course, the students are expected to be able to 1) build baseline models for real world data analysis problems; 2) implement models using programming languages; 3) draw insights/conclusions from models.\n\n\nPrerequisites\nMATH 53 and 54 or equivalents; MATH 110 is highly recommended. STAT 135 or equivalent (DATA/STAT C100 and (STAT 134 or DATA/STAT C140) will be accepted). STAT 133 preferred. STAT 151A is recommended. Scripting language required and Python experience recommended.\nReview of Matrix Algebra and Calculus, Basic Probability (Adopted from CS229 at Stanford).",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "[UNDER CONSTRUCTION]\nWe can just embed the iframe html:"
  }
]